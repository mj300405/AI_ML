{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Michal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import swifter\n",
    "import faiss\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, DPRQuestionEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "from statistics import median\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "\n",
    "nltk.download('popular')\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('medium.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Process the text through spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Preserve named entities as they are\n",
    "        if token.ent_type_:\n",
    "            processed_tokens.append(token.text)\n",
    "        # Preserve nouns and certain POS tags, exclude stopwords and punctuation\n",
    "        elif token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and not token.is_punct:\n",
    "            processed_tokens.append(token.text)\n",
    "        # Apply lemmatization and lowercasing to other tokens\n",
    "        else:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                processed_tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_article(text, max_chunk_length=512):\n",
    "    \"\"\"\n",
    "    Splits the article into manageable chunks, each not exceeding the specified maximum length.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The article text to be chunked.\n",
    "    max_chunk_length (int): The maximum allowed length of each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Split the article into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    # Further split into sentences if needed, based on the heuristic like length\n",
    "    chunks = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # Handle the case where a single sentence is longer than the max chunk length\n",
    "            if sentence_length > max_chunk_length:\n",
    "                if current_chunk:  # If the current chunk is not empty, add it to the chunks list\n",
    "                    chunks.append(' '.join(current_chunk))\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "                # Here you could further split the sentence or truncate it to fit the max length\n",
    "                # For simplicity, we'll add the long sentence as its own chunk\n",
    "                chunks.append(sentence)\n",
    "                continue\n",
    "            \n",
    "            if current_length + sentence_length > max_chunk_length:\n",
    "                # If this sentence would exceed the max length, add the current chunk first\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # After processing all sentences in a paragraph, add the remaining current chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze text distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Sentence Count: 28\n",
      "Median Word Count per Sentence: 20\n"
     ]
    }
   ],
   "source": [
    "# get text distribution to determine the max_chunk_length\n",
    "def analyze_text_distribution(texts):\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    \n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\", \"tagger\"]):\n",
    "        sentences = list(doc.sents)\n",
    "        sentence_lengths.append(len(sentences))\n",
    "        words_per_sentence = [len(sentence) for sentence in sentences]\n",
    "        word_lengths.extend(words_per_sentence)\n",
    "    \n",
    "    return sentence_lengths, word_lengths\n",
    "\n",
    "sentence_lengths, word_lengths = analyze_text_distribution(df['Text'])\n",
    "\n",
    "print(f\"Median Sentence Count: {median(sentence_lengths)}\")\n",
    "print(f\"Median Word Count per Sentence: {median(word_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHUNK_LENGTH = 20 * 28  # ~28 sentences per chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1391/1391 [06:45<00:00,  3.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1391/1391 [00:03<00:00, 348.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1391/1391 [00:01<00:00, 940.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the chunk_article function to the DataFrame\n",
    "df['processed_text'] = df['Text'].swifter.apply(preprocess_text)\n",
    "df['original_chunks'] = df['Text'].swifter.apply(lambda text: chunk_article(text, MAX_CHUNK_LENGTH))\n",
    "df['preprocessed_chunks'] = df['processed_text'].swifter.apply(lambda text: chunk_article(text, MAX_CHUNK_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>original_chunks</th>\n",
       "      <th>preprocessed_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>1 Introduction Word2vec \\n\\n Word2vec popular ...</td>\n",
       "      <td>[1. Introduction of Word2vec, Word2vec is one ...</td>\n",
       "      <td>[1 Introduction Word2vec,  Word2vec popular te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>article introduce concept Graph Neural Network...</td>\n",
       "      <td>[In my last article, I introduced the concept ...</td>\n",
       "      <td>[article introduce concept Graph Neural Networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>Introduction \\n\\n Thanks strict implementation...</td>\n",
       "      <td>[Introduction, Thanks to its strict implementa...</td>\n",
       "      <td>[Introduction,  Thanks strict implementation g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>Photo credit Mika Baumeister Unsplash \\n\\n wor...</td>\n",
       "      <td>[Photo credit to Mika Baumeister from Unsplash...</td>\n",
       "      <td>[Photo credit Mika Baumeister Unsplash,  work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>Step Step Implementation Gradient Descent Back...</td>\n",
       "      <td>[A Step-by-Step Implementation of Gradient Des...</td>\n",
       "      <td>[Step Step Implementation Gradient Descent Bac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1  In my last article, I introduced the concept o...   \n",
       "2  Introduction\\n\\nThanks to its strict implement...   \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  1 Introduction Word2vec \\n\\n Word2vec popular ...   \n",
       "1  article introduce concept Graph Neural Network...   \n",
       "2  Introduction \\n\\n Thanks strict implementation...   \n",
       "3  Photo credit Mika Baumeister Unsplash \\n\\n wor...   \n",
       "4  Step Step Implementation Gradient Descent Back...   \n",
       "\n",
       "                                     original_chunks  \\\n",
       "0  [1. Introduction of Word2vec, Word2vec is one ...   \n",
       "1  [In my last article, I introduced the concept ...   \n",
       "2  [Introduction, Thanks to its strict implementa...   \n",
       "3  [Photo credit to Mika Baumeister from Unsplash...   \n",
       "4  [A Step-by-Step Implementation of Gradient Des...   \n",
       "\n",
       "                                 preprocessed_chunks  \n",
       "0  [1 Introduction Word2vec,  Word2vec popular te...  \n",
       "1  [article introduce concept Graph Neural Networ...  \n",
       "2  [Introduction,  Thanks strict implementation g...  \n",
       "3  [Photo credit Mika Baumeister Unsplash,  work ...  \n",
       "4  [Step Step Implementation Gradient Descent Bac...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1391/1391 [24:02<00:00,  1.04s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "em_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example: Generating embeddings for preprocessed chunks\n",
    "df['embeddings'] = df['preprocessed_chunks'].swifter.apply(lambda chunks: np.array(em_model.encode(chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the embedding: <class 'numpy.ndarray'>\n",
      "Shape of the embedding: (123, 384)\n"
     ]
    }
   ],
   "source": [
    "# Assuming `df['embeddings']` is where your embeddings are stored\n",
    "sample_embedding = df['embeddings'].iloc[0]\n",
    "\n",
    "print(\"Type of the embedding:\", type(sample_embedding))\n",
    "print(\"Shape of the embedding:\", sample_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of all embeddings: (49227, 384)\n"
     ]
    }
   ],
   "source": [
    "# Flatten embeddings if they are nested\n",
    "all_embeddings = np.vstack(df['embeddings'].tolist())\n",
    "\n",
    "# Check the new shape of all embeddings\n",
    "print(\"New shape of all embeddings:\", all_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all_embeddings: (49227, 384)\n",
      "Embeddings look good!\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of all_embeddings:\", all_embeddings.shape)\n",
    "if np.isnan(all_embeddings).any():\n",
    "    print(\"Warning: NaN values found in embeddings.\")\n",
    "else:\n",
    "    print(\"Embeddings look good!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>original_chunks</th>\n",
       "      <th>preprocessed_chunks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>1 Introduction Word2vec \\n\\n Word2vec popular ...</td>\n",
       "      <td>[1. Introduction of Word2vec, Word2vec is one ...</td>\n",
       "      <td>[1 Introduction Word2vec,  Word2vec popular te...</td>\n",
       "      <td>[[-0.05949667, -0.064553834, 0.014287822, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>article introduce concept Graph Neural Network...</td>\n",
       "      <td>[In my last article, I introduced the concept ...</td>\n",
       "      <td>[article introduce concept Graph Neural Networ...</td>\n",
       "      <td>[[-0.08080695, -0.051156927, -0.0053151087, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>Introduction \\n\\n Thanks strict implementation...</td>\n",
       "      <td>[Introduction, Thanks to its strict implementa...</td>\n",
       "      <td>[Introduction,  Thanks strict implementation g...</td>\n",
       "      <td>[[-0.045066487, 0.058423676, -0.023994647, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>Photo credit Mika Baumeister Unsplash \\n\\n wor...</td>\n",
       "      <td>[Photo credit to Mika Baumeister from Unsplash...</td>\n",
       "      <td>[Photo credit Mika Baumeister Unsplash,  work ...</td>\n",
       "      <td>[[-0.12282324, 0.079836145, 0.009691512, 0.071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>Step Step Implementation Gradient Descent Back...</td>\n",
       "      <td>[A Step-by-Step Implementation of Gradient Des...</td>\n",
       "      <td>[Step Step Implementation Gradient Descent Bac...</td>\n",
       "      <td>[[-0.10204862, 0.010278484, -0.00064166903, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1  In my last article, I introduced the concept o...   \n",
       "2  Introduction\\n\\nThanks to its strict implement...   \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  1 Introduction Word2vec \\n\\n Word2vec popular ...   \n",
       "1  article introduce concept Graph Neural Network...   \n",
       "2  Introduction \\n\\n Thanks strict implementation...   \n",
       "3  Photo credit Mika Baumeister Unsplash \\n\\n wor...   \n",
       "4  Step Step Implementation Gradient Descent Back...   \n",
       "\n",
       "                                     original_chunks  \\\n",
       "0  [1. Introduction of Word2vec, Word2vec is one ...   \n",
       "1  [In my last article, I introduced the concept ...   \n",
       "2  [Introduction, Thanks to its strict implementa...   \n",
       "3  [Photo credit to Mika Baumeister from Unsplash...   \n",
       "4  [A Step-by-Step Implementation of Gradient Des...   \n",
       "\n",
       "                                 preprocessed_chunks  \\\n",
       "0  [1 Introduction Word2vec,  Word2vec popular te...   \n",
       "1  [article introduce concept Graph Neural Networ...   \n",
       "2  [Introduction,  Thanks strict implementation g...   \n",
       "3  [Photo credit Mika Baumeister Unsplash,  work ...   \n",
       "4  [Step Step Implementation Gradient Descent Bac...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.05949667, -0.064553834, 0.014287822, 0.03...  \n",
       "1  [[-0.08080695, -0.051156927, -0.0053151087, -0...  \n",
       "2  [[-0.045066487, 0.058423676, -0.023994647, 0.0...  \n",
       "3  [[-0.12282324, 0.079836145, 0.009691512, 0.071...  \n",
       "4  [[-0.10204862, 0.010278484, -0.00064166903, -0...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 384  # Dimensionality of the embeddings\n",
    "index = faiss.IndexFlatL2(d)  # Using L2 distance for the similarity measure\n",
    "\n",
    "# Assuming `all_embeddings` is your numpy array of shape (49227, 384)\n",
    "index.add(all_embeddings)  # Add your embeddings to the index\n",
    "\n",
    "# Optionally, save the index to disk for later use\n",
    "faiss.write_index(index, \"index.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>original_chunks</th>\n",
       "      <th>preprocessed_chunks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "      <td>1 Introduction Word2vec \\n\\n Word2vec popular ...</td>\n",
       "      <td>[1. Introduction of Word2vec, Word2vec is one ...</td>\n",
       "      <td>[1 Introduction Word2vec,  Word2vec popular te...</td>\n",
       "      <td>[[-0.05949667, -0.064553834, 0.014287822, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "      <td>article introduce concept Graph Neural Network...</td>\n",
       "      <td>[In my last article, I introduced the concept ...</td>\n",
       "      <td>[article introduce concept Graph Neural Networ...</td>\n",
       "      <td>[[-0.08080695, -0.051156927, -0.0053151087, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "      <td>Introduction \\n\\n Thanks strict implementation...</td>\n",
       "      <td>[Introduction, Thanks to its strict implementa...</td>\n",
       "      <td>[Introduction,  Thanks strict implementation g...</td>\n",
       "      <td>[[-0.045066487, 0.058423676, -0.023994647, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "      <td>Photo credit Mika Baumeister Unsplash \\n\\n wor...</td>\n",
       "      <td>[Photo credit to Mika Baumeister from Unsplash...</td>\n",
       "      <td>[Photo credit Mika Baumeister Unsplash,  work ...</td>\n",
       "      <td>[[-0.12282324, 0.079836145, 0.009691512, 0.071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>Step Step Implementation Gradient Descent Back...</td>\n",
       "      <td>[A Step-by-Step Implementation of Gradient Des...</td>\n",
       "      <td>[Step Step Implementation Gradient Descent Bac...</td>\n",
       "      <td>[[-0.10204862, 0.010278484, -0.00064166903, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...   \n",
       "1  In my last article, I introduced the concept o...   \n",
       "2  Introduction\\n\\nThanks to its strict implement...   \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  1 Introduction Word2vec \\n\\n Word2vec popular ...   \n",
       "1  article introduce concept Graph Neural Network...   \n",
       "2  Introduction \\n\\n Thanks strict implementation...   \n",
       "3  Photo credit Mika Baumeister Unsplash \\n\\n wor...   \n",
       "4  Step Step Implementation Gradient Descent Back...   \n",
       "\n",
       "                                     original_chunks  \\\n",
       "0  [1. Introduction of Word2vec, Word2vec is one ...   \n",
       "1  [In my last article, I introduced the concept ...   \n",
       "2  [Introduction, Thanks to its strict implementa...   \n",
       "3  [Photo credit to Mika Baumeister from Unsplash...   \n",
       "4  [A Step-by-Step Implementation of Gradient Des...   \n",
       "\n",
       "                                 preprocessed_chunks  \\\n",
       "0  [1 Introduction Word2vec,  Word2vec popular te...   \n",
       "1  [article introduce concept Graph Neural Networ...   \n",
       "2  [Introduction,  Thanks strict implementation g...   \n",
       "3  [Photo credit Mika Baumeister Unsplash,  work ...   \n",
       "4  [Step Step Implementation Gradient Descent Bac...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.05949667, -0.064553834, 0.014287822, 0.03...  \n",
       "1  [[-0.08080695, -0.051156927, -0.0053151087, -0...  \n",
       "2  [[-0.045066487, 0.058423676, -0.023994647, 0.0...  \n",
       "3  [[-0.12282324, 0.079836145, 0.009691512, 0.071...  \n",
       "4  [[-0.10204862, 0.010278484, -0.00064166903, -0...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 49227/49227 [00:00<00:00, 666566.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare a list to hold your new rows including flattened chunks and embeddings\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over each row in your original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    for chunk, embedding in zip(row['original_chunks'], row['embeddings']):\n",
    "        # Create a new row for each chunk, including the title, chunk text (as 'text'), and embedding\n",
    "        new_rows.append({'title': row['Title'], 'text': chunk, 'embeddings': embedding})\n",
    "\n",
    "# Convert the list of new rows into a DataFrame\n",
    "flattened_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Convert the flattened DataFrame into a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(flattened_df)\n",
    "\n",
    "# Optionally, save this dataset to disk for later use or inspection\n",
    "hf_dataset.save_to_disk('./dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec</td>\n",
       "      <td>[-0.05949667, -0.064553834, 0.014287822, 0.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>Word2vec is one of the most popular technique ...</td>\n",
       "      <td>[-0.012309051, -0.12891136, -0.02354605, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>Word embedding via word2vec can make natural l...</td>\n",
       "      <td>[-0.010934148, -0.051672615, -0.05682393, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>For instance, the words women, men, and human ...</td>\n",
       "      <td>[-0.09067625, 0.012690775, -0.06001801, -0.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>There are two main training algorithms for wor...</td>\n",
       "      <td>[-0.008359772, -0.013869865, -0.01232508, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "2  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "3  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "4  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "\n",
       "                                                text  \\\n",
       "0                        1. Introduction of Word2vec   \n",
       "1  Word2vec is one of the most popular technique ...   \n",
       "2  Word embedding via word2vec can make natural l...   \n",
       "3  For instance, the words women, men, and human ...   \n",
       "4  There are two main training algorithms for wor...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.05949667, -0.064553834, 0.014287822, 0.031...  \n",
       "1  [-0.012309051, -0.12891136, -0.02354605, 0.033...  \n",
       "2  [-0.010934148, -0.051672615, -0.05682393, 0.00...  \n",
       "3  [-0.09067625, 0.012690775, -0.06001801, -0.014...  \n",
       "4  [-0.008359772, -0.013869865, -0.01232508, 0.00...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file for RAG's use\n",
    "flattened_df.to_csv('rag_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 49227/49227 [00:00<00:00, 769413.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(flattened_df)\n",
    "\n",
    "# Save the dataset to disk for RAG to use\n",
    "dataset.save_to_disk(\"./dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:180: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "c:\\Users\\Michal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# Initialize the question tokenizer\n",
    "dpr_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=\"./dataset\",\n",
    "    index_path=\"index.faiss\"\n",
    ")\n",
    "\n",
    "# Initialize the RAG model\n",
    "model = RagSequenceForGeneration.from_pretrained(\n",
    "    \"facebook/rag-token-nq\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to load your spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your preprocessing function\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_:\n",
    "            processed_tokens.append(token.text)\n",
    "        elif token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and not token.is_punct:\n",
    "            processed_tokens.append(token.text)\n",
    "        else:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                processed_tokens.append(token.lemma_.lower())\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Your embedding model\n",
    "em_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to preprocess and embed the query\n",
    "def get_query_embedding(query):\n",
    "    processed_query = preprocess_text(query)\n",
    "    return em_model.encode([processed_query])[0]  # Encode returns a list of embeddings\n",
    "\n",
    "# Load your FAISS index\n",
    "faiss_index = faiss.read_index(\"index.faiss\")\n",
    "\n",
    "# Load your Hugging Face dataset\n",
    "hf_dataset = load_from_disk(\"./dataset\")\n",
    "\n",
    "# Function to search the index with the query embedding\n",
    "def search(query, k=5):\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]).astype(\"float32\"), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Function to retrieve chunks from the dataset\n",
    "def get_retrieved_chunks(indices):\n",
    "    return [hf_dataset[int(idx)] for idx in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(query, num_results=5, combine_chunks=True, max_combined_length=1024):\n",
    "    distances, indices = search(query, k=num_results)\n",
    "    retrieved_data = get_retrieved_chunks(indices)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    for i, data in enumerate(retrieved_data):\n",
    "        combined_text = data['text']\n",
    "        next_idx = i + 1\n",
    "        \n",
    "        # Combine chunks from the same article if they are consecutive\n",
    "        while combine_chunks and next_idx < len(retrieved_data) and len(combined_text) < max_combined_length:\n",
    "            if retrieved_data[next_idx]['title'] == data['title']:\n",
    "                combined_text += ' ' + retrieved_data[next_idx]['text']\n",
    "                next_idx += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"Result {i+1}: (Score: {1 - distances[0][i]:.4f})\")\n",
    "        print(f\"Title: {data['title']}\\nText: {combined_text}\\n\")\n",
    "        if next_idx > i + 1:\n",
    "            break  # Break the loop if we have combined chunks so we don't print the same chunks again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Machine Learning\n",
      "\n",
      "Result 1: (Score: 1.0000)\n",
      "Title: On the Journey to Machine Learning / AI\n",
      "Text: What is Machine Learning?\n",
      "\n",
      "Result 2: (Score: 1.0000)\n",
      "Title: So what is Machine Learning?\n",
      "Text: You can easily find many popular use-cases of Machine Learning. I am sure you check Amazon for when you need to buy new clothes or shoes. And then you see a list of recommended items for you. This is, in fact, machine learning at play.\n",
      "\n",
      "Result 3: (Score: 1.0000)\n",
      "Title: Practical Machine Learning with C++ and GRT\n",
      "Text: What is machine learning?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Machine Learning\"\n",
    "number_of_articles_to_retrieve = 3\n",
    "display_results(query, number_of_articles_to_retrieve, combine_chunks=True, max_combined_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "import spacy\n",
      "import swifter\n",
      "import faiss\n",
      "from collections import defaultdict\n",
      "from sentence_transformers import SentenceTransformer\n",
      "from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, DPRQuestionEncoderTokenizer\n",
      "from datasets import load_dataset\n",
      "from statistics import median\n",
      "from datasets import Dataset, load_from_disk\n",
      "\n",
      "\n",
      "nltk.download('popular')\n",
      "spacy.cli.download(\"en_core_web_sm\")\n",
      "# Load the dataset\n",
      "df = pd.read_csv('medium.csv')\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "def preprocess_text(text):\n",
      "    # Process the text through spaCy NLP pipeline\n",
      "    doc = nlp(text)\n",
      "    processed_tokens = []\n",
      "    \n",
      "    for token in doc:\n",
      "        # Preserve named entities as they are\n",
      "        if token.ent_type_:\n",
      "            processed_tokens.append(token.text)\n",
      "        # Preserve nouns and certain POS tags, exclude stopwords and punctuation\n",
      "        elif token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and not token.is_punct:\n",
      "            processed_tokens.append(token.text)\n",
      "        # Apply lemmatization and lowercasing to other tokens\n",
      "        else:\n",
      "            if not token.is_stop and not token.is_punct:\n",
      "                processed_tokens.append(token.lemma_.lower())\n",
      "    \n",
      "    return ' '.join(processed_tokens)\n",
      "\n",
      "def chunk_article(text, max_chunk_length=512):\n",
      "    \"\"\"\n",
      "    Splits the article into manageable chunks, each not exceeding the specified maximum length.\n",
      "    \n",
      "    Args:\n",
      "    text (str): The article text to be chunked.\n",
      "    max_chunk_length (int): The maximum allowed length of each chunk.\n",
      "    \n",
      "    Returns:\n",
      "    list: A list of text chunks.\n",
      "    \"\"\"\n",
      "    # Split the article into paragraphs\n",
      "    paragraphs = text.split('\\n\\n')\n",
      "    \n",
      "    # Further split into sentences if needed, based on the heuristic like length\n",
      "    chunks = []\n",
      "    for paragraph in paragraphs:\n",
      "        sentences = nltk.sent_tokenize(paragraph)\n",
      "        current_chunk = []\n",
      "        current_length = 0\n",
      "        \n",
      "        for sentence in sentences:\n",
      "            sentence_length = len(sentence)\n",
      "            \n",
      "            # Handle the case where a single sentence is longer than the max chunk length\n",
      "            if sentence_length > max_chunk_length:\n",
      "                if current_chunk:  # If the current chunk is not empty, add it to the chunks list\n",
      "                    chunks.append(' '.join(current_chunk))\n",
      "                    current_chunk = []\n",
      "                    current_length = 0\n",
      "                # Here you could further split the sentence or truncate it to fit the max length\n",
      "                # For simplicity, we'll add the long sentence as its own chunk\n",
      "                chunks.append(sentence)\n",
      "                continue\n",
      "            \n",
      "            if current_length + sentence_length > max_chunk_length:\n",
      "                # If this sentence would exceed the max length, add the current chunk first\n",
      "                chunks.append(' '.join(current_chunk))\n",
      "                current_chunk = [sentence]\n",
      "                current_length = sentence_length\n",
      "            else:\n",
      "                current_chunk.append(sentence)\n",
      "                current_length += sentence_length\n",
      "        \n",
      "        # After processing all sentences in a paragraph, add the remaining current chunk\n",
      "        if current_chunk:\n",
      "            chunks.append(' '.join(current_chunk))\n",
      "    \n",
      "    return chunks\n",
      "# get text distribution to determine the max_chunk_length\n",
      "def analyze_text_distribution(texts):\n",
      "    sentence_lengths = []\n",
      "    word_lengths = []\n",
      "    \n",
      "    for doc in nlp.pipe(texts, disable=[\"ner\", \"tagger\"]):\n",
      "        sentences = list(doc.sents)\n",
      "        sentence_lengths.append(len(sentences))\n",
      "        words_per_sentence = [len(sentence) for sentence in sentences]\n",
      "        word_lengths.extend(words_per_sentence)\n",
      "    \n",
      "    return sentence_lengths, word_lengths\n",
      "\n",
      "sentence_lengths, word_lengths = analyze_text_distribution(df['Text'])\n",
      "\n",
      "print(f\"Median Sentence Count: {median(sentence_lengths)}\")\n",
      "print(f\"Median Word Count per Sentence: {median(word_lengths)}\")\n",
      "MAX_CHUNK_LENGTH = 20 * 28  # ~28 sentences per chunk\n",
      "# apply the chunk_article function to the DataFrame\n",
      "df['processed_text'] = df['Text'].swifter.apply(preprocess_text)\n",
      "df['original_chunks'] = df['Text'].swifter.apply(lambda text: chunk_article(text, MAX_CHUNK_LENGTH))\n",
      "df['preprocessed_chunks'] = df['processed_text'].swifter.apply(lambda text: chunk_article(text, MAX_CHUNK_LENGTH))\n",
      "df.head()\n",
      "# Load a pre-trained model\n",
      "em_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "\n",
      "# Example: Generating embeddings for preprocessed chunks\n",
      "df['embeddings'] = df['preprocessed_chunks'].swifter.apply(lambda chunks: np.array(em_model.encode(chunks)))\n",
      "# Assuming `df['embeddings']` is where your embeddings are stored\n",
      "sample_embedding = df['embeddings'].iloc[0]\n",
      "\n",
      "print(\"Type of the embedding:\", type(sample_embedding))\n",
      "print(\"Shape of the embedding:\", sample_embedding.shape)\n",
      "\n",
      "# Flatten embeddings if they are nested\n",
      "all_embeddings = np.vstack(df['embeddings'].tolist())\n",
      "\n",
      "# Check the new shape of all embeddings\n",
      "print(\"New shape of all embeddings:\", all_embeddings.shape)\n",
      "\n",
      "print(\"Shape of all_embeddings:\", all_embeddings.shape)\n",
      "if np.isnan(all_embeddings).any():\n",
      "    print(\"Warning: NaN values found in embeddings.\")\n",
      "else:\n",
      "    print(\"Embeddings look good!\")\n",
      "\n",
      "df.head()\n",
      "d = 384  # Dimensionality of the embeddings\n",
      "index = faiss.IndexFlatL2(d)  # Using L2 distance for the similarity measure\n",
      "\n",
      "# Assuming `all_embeddings` is your numpy array of shape (49227, 384)\n",
      "index.add(all_embeddings)  # Add your embeddings to the index\n",
      "\n",
      "# Optionally, save the index to disk for later use\n",
      "faiss.write_index(index, \"index.faiss\")\n",
      "\n",
      "df.head()\n",
      "# Prepare a list to hold your new rows including flattened chunks and embeddings\n",
      "new_rows = []\n",
      "\n",
      "# Iterate over each row in your original DataFrame\n",
      "for _, row in df.iterrows():\n",
      "    for chunk, embedding in zip(row['original_chunks'], row['embeddings']):\n",
      "        # Create a new row for each chunk, including the title, chunk text (as 'text'), and embedding\n",
      "        new_rows.append({'title': row['Title'], 'text': chunk, 'embeddings': embedding})\n",
      "\n",
      "# Convert the list of new rows into a DataFrame\n",
      "flattened_df = pd.DataFrame(new_rows)\n",
      "\n",
      "# Convert the flattened DataFrame into a Hugging Face Dataset\n",
      "hf_dataset = Dataset.from_pandas(flattened_df)\n",
      "\n",
      "# Optionally, save this dataset to disk for later use or inspection\n",
      "hf_dataset.save_to_disk('./dataset')\n",
      "\n",
      "flattened_df.head()\n",
      "# Save the DataFrame to a CSV file for RAG's use\n",
      "flattened_df.to_csv('rag_dataset.csv', index=False)\n",
      "# Convert the DataFrame to a Hugging Face dataset\n",
      "dataset = Dataset.from_pandas(flattened_df)\n",
      "\n",
      "# Save the dataset to disk for RAG to use\n",
      "dataset.save_to_disk(\"./dataset\")\n",
      "# Initialize the tokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
      "\n",
      "# Initialize the question tokenizer\n",
      "dpr_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
      "\n",
      "# Initialize the retriever\n",
      "retriever = RagRetriever.from_pretrained(\n",
      "    \"facebook/rag-token-nq\",\n",
      "    index_name=\"custom\",\n",
      "    passages_path=\"./dataset\",\n",
      "    index_path=\"index.faiss\"\n",
      ")\n",
      "\n",
      "# Initialize the RAG model\n",
      "model = RagSequenceForGeneration.from_pretrained(\n",
      "    \"facebook/rag-token-nq\",\n",
      "    retriever=retriever\n",
      ")\n",
      "# Make sure to load your spaCy model\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Your preprocessing function\n",
      "def preprocess_text(text):\n",
      "    doc = nlp(text)\n",
      "    processed_tokens = []\n",
      "    for token in doc:\n",
      "        if token.ent_type_:\n",
      "            processed_tokens.append(token.text)\n",
      "        elif token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and not token.is_punct:\n",
      "            processed_tokens.append(token.text)\n",
      "        else:\n",
      "            if not token.is_stop and not token.is_punct:\n",
      "                processed_tokens.append(token.lemma_.lower())\n",
      "    return ' '.join(processed_tokens)\n",
      "\n",
      "# Your embedding model\n",
      "em_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
      "\n",
      "# Function to preprocess and embed the query\n",
      "def get_query_embedding(query):\n",
      "    processed_query = preprocess_text(query)\n",
      "    return em_model.encode([processed_query])[0]  # Encode returns a list of embeddings\n",
      "\n",
      "# Load your FAISS index\n",
      "faiss_index = faiss.read_index(\"index.faiss\")\n",
      "\n",
      "# Load your Hugging Face dataset\n",
      "hf_dataset = load_from_disk(\"./dataset\")\n",
      "\n",
      "# Function to search the index with the query embedding\n",
      "def search(query, k=5):\n",
      "    query_embedding = get_query_embedding(query)\n",
      "    distances, indices = faiss_index.search(np.array([query_embedding]).astype(\"float32\"), k)\n",
      "    return distances, indices\n",
      "\n",
      "# Function to retrieve chunks from the dataset\n",
      "def get_retrieved_chunks(indices):\n",
      "    return [hf_dataset[int(idx)] for idx in indices[0]]\n",
      "def display_results(query, num_results=5, combine_chunks=True, max_combined_length=1024):\n",
      "    distances, indices = search(query, k=num_results)\n",
      "    retrieved_data = get_retrieved_chunks(indices)\n",
      "    \n",
      "    print(f\"Query: {query}\\n\")\n",
      "    for i, data in enumerate(retrieved_data):\n",
      "        combined_text = data['text']\n",
      "        next_idx = i + 1\n",
      "        \n",
      "        # Combine chunks from the same article if they are consecutive\n",
      "        while combine_chunks and next_idx < len(retrieved_data) and len(combined_text) < max_combined_length:\n",
      "            if retrieved_data[next_idx]['title'] == data['title']:\n",
      "                combined_text += ' ' + retrieved_data[next_idx]['text']\n",
      "                next_idx += 1\n",
      "            else:\n",
      "                break\n",
      "        \n",
      "        print(f\"Result {i+1}: (Score: {1 - distances[0][i]:.4f})\")\n",
      "        print(f\"Title: {data['title']}\\nText: {combined_text}\\n\")\n",
      "        if next_idx > i + 1:\n",
      "            break  # Break the loop if we have combined chunks so we don't print the same chunks again\n",
      "# Example usage\n",
      "query = \"Machine Learning\"\n",
      "number_of_articles_to_retrieve = 3\n",
      "display_results(query, number_of_articles_to_retrieve, combine_chunks=True, max_combined_length=1024)\n",
      "import nbformat\n",
      "\n",
      "# Load the notebook\n",
      "notebook_path = 'solution.ipynb'\n",
      "with open(notebook_path, 'r', encoding='utf-8') as nb_file:\n",
      "    nb_content = nbformat.read(nb_file, as_version=4)\n",
      "\n",
      "# Extract and print the code cells' content for review\n",
      "for cell in nb_content['cells']:\n",
      "    if cell['cell_type'] == 'code':\n",
      "        print(cell['source'])\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "notebook_path = 'solution.ipynb'\n",
    "with open(notebook_path, 'r', encoding='utf-8') as nb_file:\n",
    "    nb_content = nbformat.read(nb_file, as_version=4)\n",
    "\n",
    "# Extract and print the code cells' content for review\n",
    "for cell in nb_content['cells']:\n",
    "    if cell['cell_type'] == 'code':\n",
    "        print(cell['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
